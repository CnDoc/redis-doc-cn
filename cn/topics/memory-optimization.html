<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8' />
    <link href='../styles.css' rel='stylesheet' type='text/css' />
    <link href='../images/favicon.png' rel='shortcut icon' />
    <link href='../opensearch.xml' rel='search' title='Look up a Redis command' type='application/opensearchdescription+xml' />
    <script src='/jquery.min.js'></script>
    <script async='async' defer='defer' src='../app.js'></script>
    <meta content='width=device-width, minimum-scale=1.0, maximum-scale=1.0' name='viewport' />
    <title>
      Redis
    </title>
    
  </head>
  <body class='topics memory-optimization'><style type="text/css">#modify_div {position: fixed;right: 0px;top: 60px;	background-color: #000000;	height: 30px;width: 100px;border-top-width: 2px;border-right-width: 2px;border-bottom-width: 2px;border-left-width: 2px;border-top-style: dashed;border-right-style: none;border-bottom-style: dashed;border-left-style: dashed;border-top-color: #333333;border-right-color: #333333;border-bottom-color: #333333;border-left-color: #333333;}#modify {display: block;position: fixed;right: 23px;top: 70px;color: #FFFFFF;text-decoration: none;font-size: 12px;font-weight: bold;}#modify:hover {text-decoration: underline;}</style><div id="modify_div"></div><a href="https://github.com/jialechan/redis-doc-cn/edit/gh-pages/cn/topics/memory-optimization.html" id="modify" target="_blank">修改本页</a><script>$('#modify_div').css('opacity', '0.6');	</script>
    <header>
      <div class='container'>
        <a href='../index.html'>
          <img alt='Redis' height='30' src='../images/redis.png' width='93' />
        </a>
        <nav>
          <a href='../commands.html'>Commands</a>
          <a href='../clients.html'>Clients</a>
          <a href='../documentation.html'>Documentation</a>
          <a href='../community.html'>Community</a>
          <a href='../download.html'>Download</a>
          <a href='https://github.com/antirez/redis/issues'>Issues</a>
          <a href='../support.html'>Support</a>
          <a href='license.html'>License</a>
        </nav>
      </div>
    </header>
    <div class='text'>
      <article id='topic'>
        <p>This page is a work in progress. Currently it is just a list of things you should check if you have problems with memory.</p>
        
        <h2>Special encoding of small aggregate data types</h2>
        
        <p>Since Redis 2.2 many data types are optimized to use less space up to a certain size. Hashes, Lists, Sets composed of just integers, and Sorted Sets, when smaller than a given number of elements, and up to a maximum element size, are encoded in a very memory efficient way that uses <em>up to 10 times less memory</em> (with 5 time less memory used being the average saving).</p>
        
        <p>This is completely transparent from the point of view of the user and API.
        Since this is a CPU / memory trade off it is possible to tune the maximum number of elements and maximum element size for special encoded types using the following redis.conf directives.</p>
        
        <pre><code>hash-max-zipmap-entries 64 (hash-max-ziplist-entries for Redis &gt;= 2.6)&#x000A;hash-max-zipmap-value 512  (hash-max-ziplist-value for Redis &gt;= 2.6)&#x000A;list-max-ziplist-entries 512&#x000A;list-max-ziplist-value 64&#x000A;zset-max-ziplist-entries 128&#x000A;zset-max-ziplist-value 64&#x000A;set-max-intset-entries 512&#x000A;</code></pre>
        
        <p>If a specially encoded value will overflow the configured max size, Redis will automatically convert it into normal encoding. This operation is very fast for small values, but if you change the setting in order to use specially encoded values for much larger aggregate types the suggestion is to run some benchmark and test to check the conversion time.</p>
        
        <h2>Using 32 bit instances</h2>
        
        <p>Redis compiled with 32 bit target uses a lot less memory per key, since pointers are small, but such an instance will be limited to 4 GB of maximum memory usage. To compile Redis as 32 bit binary use <em>make 32bit</em>. RDB and AOF files are compatible between 32 bit and 64 bit instances (and between little and big endian of course) so you can switch from 32 to 64 bit, or the contrary, without problems.</p>
        
        <h2>Bit and byte level operations</h2>
        
        <p>Redis 2.2 introduced new bit and byte level operations: <a href="../commands/getrange.html">GETRANGE</a>, <a href="../commands/setrange.html">SETRANGE</a>, <a href="../commands/getbit.html">GETBIT</a> and <a href="../commands/setbit.html">SETBIT</a>. Using this commands you can treat the Redis string type as a random access array. For instance if you have an application where users are identified by an unique progressive integer number, you can use a bitmap in order to save information about sex of users, setting the bit for females and clearing it for males, or the other way around. With 100 millions of users this data will take just 12 megabyte of RAM in a Redis instance. You can do the same using <a href="../commands/getrange.html">GETRANGE</a> and <a href="../commands/setrange.html">SETRANGE</a> in order to store one byte of information for user. This is just an example but it is actually possible to model a number of problems in very little space with this new primitives.</p>
        
        <h2>Use hashes when possible</h2>
        
        <p>Small hashes are encoded in a very small space, so you should try representing your data using hashes every time it is possible. For instance if you have objects representing users in a web application, instead of using different keys for name, surname, email, password, use a single hash with all the required fields.</p>
        
        <p>If you want to know more about this, read the next section.</p>
        
        <h2>Using hashes to abstract a very memory efficient plain key-value store on top of Redis</h2>
        
        <p>I understand the title of this section is a bit scaring, but I&#39;m going to explain in details what this is about.</p>
        
        <p>Basically it is possible to model a plain key-value store using Redis
        where values can just be just strings, that is not just more memory efficient
        than Redis plain keys but also much more memory efficient than memcached.</p>
        
        <p>Let&#39;s start with some fact: a few keys use a lot more memory than a single key
        containing a hash with a few fields. How is this possible? We use a trick.
        In theory in order to guarantee that we perform lookups in constant time
        (also known as <span class="math">O(1) </span>in big O notation) there is the need to use a data structure
        with a constant time complexity in the average case, like a hash table.</p>
        
        <p>But many times hashes contain just a few fields. When hashes are small we can
        instead just encode them in an <span class="math">O(N) </span>data structure, like a linear
        array with length-prefixed key value pairs. Since we do this only when N
        is small, the amortized time for HGET and HSET commands is still O(1): the
        hash will be converted into a real hash table as soon as the number of elements
        it contains will grow too much (you can configure the limit in redis.conf).</p>
        
        <p>This does not work well just from the point of view of time complexity, but
        also from the point of view of constant times, since a linear array of key
        value pairs happens to play very well with the CPU cache (it has a better
        cache locality than a hash table).</p>
        
        <p>However since hash fields and values are not (always) represented as full
        featured Redis objects, hash fields can&#39;t have an associated time to live
        (expire) like a real key, and can only contain a string. But we are okay with
        this, this was anyway the intention when the hash data type API was
        designed (we trust simplicity more than features, so nested data structures
        are not allowed, as expires of single fields are not allowed).</p>
        
        <p>So hashes are memory efficient. This is very useful when using hashes
        to represent objects or to model other problems when there are group of
        related fields. But what about if we have a plain key value business?</p>
        
        <p>Imagine we want to use Redis as a cache for many small objects, that can be
        JSON encoded objects, small HTML fragments, simple key -&gt; boolean values
        and so forth. Basically anything is a string -&gt; string map with small keys
        and values.</p>
        
        <p>Now let&#39;s assume the objects we want to cache are numbered, like:</p>
        
        <ul>
        <li>object:102393</li>
        <li>object:1234</li>
        <li>object:5</li>
        </ul>
        
        <p>This is what we can do. Every time there is to perform a
        SET operation to set a new value, we actually split the key into two parts,
        one used as a key, and used as field name for the hash. For instance the
        object named &quot;object:1234&quot; is actually split into:</p>
        
        <ul>
        <li>a Key named object:12</li>
        <li>a Field named 34</li>
        </ul>
        
        <p>So we use all the characters but the latest two for the key, and the final
        two characters for the hash field name. To set our key we use the following
        command:</p>
        
        <pre><code>HSET object:12 34 somevalue&#x000A;</code></pre>
        
        <p>As you can see every hash will end containing 100 fields, that
        is an optimal compromise between CPU and memory saved.</p>
        
        <p>There is another very important thing to note, with this schema
        every hash will have more or
        less 100 fields regardless of the number of objects we cached. This is since
        our objects will always end with a number, and not a random string. In some
        way the final number can be considered as a form of implicit pre-sharding.</p>
        
        <p>What about small numbers? Like object:2? We handle this case using just
        &quot;object:&quot; as a key name, and the whole number as the hash field name.
        So object:2 and object:10 will both end inside the key &quot;object:&quot;, but one
        as field name &quot;2&quot; and one as &quot;10&quot;.</p>
        
        <p>How much memory we save this way?</p>
        
        <p>I used the following Ruby program to test how this works:</p>
        
        <pre><code>require &#39;rubygems&#39;&#x000A;require &#39;redis&#39;&#x000A;&#x000A;UseOptimization = true&#x000A;&#x000A;def hash_get_key_field(key)&#x000A;    s = key.split(&quot;:&quot;)&#x000A;    if s[1].length &gt; 2&#x000A;        {:key =&gt; s[0]+&quot;:&quot;+s[1][0..-3], :field =&gt; s[1][-2..-1]}&#x000A;    else&#x000A;        {:key =&gt; s[0]+&quot;:&quot;, :field =&gt; s[1]}&#x000A;    end&#x000A;end&#x000A;&#x000A;def hash_set(r,key,value)&#x000A;    kf = hash_get_key_field(key)&#x000A;    r.hset(kf[:key],kf[:field],value)&#x000A;end&#x000A;&#x000A;def hash_get(r,key,value)&#x000A;    kf = hash_get_key_field(key)&#x000A;    r.hget(kf[:key],kf[:field],value)&#x000A;end&#x000A;&#x000A;r = Redis.new&#x000A;(0..100000).each{|id|&#x000A;    key = &quot;object:#{id}&quot;&#x000A;    if UseOptimization&#x000A;        hash_set(r,key,&quot;val&quot;)&#x000A;    else&#x000A;        r.set(key,&quot;val&quot;)&#x000A;    end&#x000A;}&#x000A;</code></pre>
        
        <p>This is the result against a 64 bit instance of Redis 2.2:</p>
        
        <ul>
        <li>UseOptimization set to true: 1.7 MB of used memory</li>
        <li>UseOptimization set to false; 11 MB of used memory</li>
        </ul>
        
        <p>This is an order of magnitude, I think this makes Redis more or less the most
        memory efficient plain key value store out there.</p>
        
        <p><em>WARNING</em>: for this to work, make sure that in your redis.conf you have
        something like this:</p>
        
        <pre><code>hash-max-zipmap-entries 256&#x000A;</code></pre>
        
        <p>Also remember to set the following field accordingly to the maximum size
        of your keys and values:</p>
        
        <pre><code>hash-max-zipmap-value 1024&#x000A;</code></pre>
        
        <p>Every time a hash will exceed the number of elements or element size specified
        it will be converted into a real hash table, and the memory saving will be lost.</p>
        
        <p>You may ask, why don&#39;t you do this implicitly in the normal key space so that
        I don&#39;t have to care? There are two reasons: one is that we tend to make
        trade offs explicit, and this is a clear tradeoff between many things: CPU,
        memory, max element size. The second is that the top level key space must
        support a lot of interesting things like expires, LRU data, and so
        forth so it is not practical to do this in a general way.</p>
        
        <p>But the Redis Way is that the user must understand how things work so that
        he is able to pick the best compromise, and to understand how the system will
        behave exactly.</p>
        
        <h2>Memory allocation</h2>
        
        <p>To store user keys, Redis allocates at most as much memory as the <code>maxmemory</code>
        setting enables (however there are small extra allocations possible).</p>
        
        <p>The exact value can be set in the configuration file or set later via
        <code>CONFIG SET</code> (see <a href="lru-cache.html">Using memory as an LRU cache for more info</a>). There are a few things that should be noted about how
        Redis manages memory:</p>
        
        <ul>
        <li>Redis will not always free up (return) memory to the OS when keys are removed.
        This is not something special about Redis, but it is how most malloc() implementations work. For example if you fill an instance with 5GB worth of data, and then
        remove the equivalent of 2GB of data, the Resident Set Size (also known as
        the RSS, which is the number of memory pages consumed by the process)
        will probably still be around 5GB, even if Redis will claim that the user
        memory is around 3GB.  This happens because the underlying allocator can&#39;t easily release the memory. For example often most of the removed keys were allocated in the same pages as the other keys that still exist.</li>
        <li>The previous point means that you need to provision memory based on your
        <strong>peak memory usage</strong>. If your workload from time to time requires 10GB, even if
        most of the times 5GB could do, you need to provision for 10GB.</li>
        <li>However allocators are smart and are able to reuse free chunks of memory,
        so after you freed 2GB of your 5GB data set, when you start adding more keys
        again, you&#39;ll see the RSS (Resident Set Size) to stay steady and don&#39;t grow
        more, as you add up to 2GB of additional keys. The allocator is basically
        trying to reuse the 2GB of memory previously (logically) freed.</li>
        <li>Because of all this, the fragmentation ratio is not reliable when you
        had a memory usage that at peak is much larger than the currently used memory.
        The fragmentation is calculated as the amount of memory currently in use
        (as the sum of all the allocations performed by Redis) divided by the physical
        memory actually used (the RSS value). Because the RSS reflects the peak memory,
        when the (virtually) used memory is low since a lot of keys / values were
        freed, but the RSS is high, the ration <code>mem_used / RSS</code> will be very high.</li>
        </ul>
        
        <p>If <code>maxmemory</code> is not set Redis will keep allocating memory as it finds
        fit and thus it can (gradually) eat up all your free memory.
        Therefore it is generally advisable to configure some limit. You may also
        want to set <code>maxmemory-policy</code> to <code>noeviction</code> (which is <em>not</em> the default
        value in some older versions of Redis).</p>
        
        <p>It makes Redis return an out of memory error for write commands if and when it reaches the limit - which in turn may result in errors in the application but will not render the whole machine dead because of memory starvation.</p>
        
        <h2>Work in progress</h2>
        
        <p>Work in progress... more tips will be added soon.</p>
      </article>
    </div>
    <footer>
      <p>
        This website is
        <a href="https://github.com/antirez/redis-io">open source software</a>
        developed by <a href="http://citrusbyte.com">Citrusbyte</a>.
        <br> The Redis logo was designed by <a href="http://www.carlosprioglio.com/">Carlos Prioglio</a>. See more <a href="sponsors.html">credits</a>.
      </p>
      <div class='sponsor'>
        Sponsored by
        <a href='http://www.gopivotal.com/products/redis'>
          <img alt='Redis Support' height='25' src='../images/pivotal.png' title='Redis Sponsor' width='99' />
        </a>
      </div>
    </footer>
  </body>
</html>

