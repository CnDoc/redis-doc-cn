<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8' />
    <link href='../styles.css%3F1333128600.css' rel='stylesheet' type='text/css' />
    <link href='../images/favicon.png' rel='shortcut icon' />
    <link href='../opensearch.xml' rel='search' title='Look up a Redis command' type='application/opensearchdescription+xml' />
    <script src='http://ajax.googleapis.com/ajax/libs/jquery/1.4/jquery.min.js'></script>
    <script async='async' defer='defer' src='../app.js'></script>
    <meta content='width=device-width, minimum-scale=1.0, maximum-scale=1.0' name='viewport' />
    <title>
      Redis cluster Specification (work in progress) – Redis
    </title>
    
  </head>
  <body class='topics cluster-spec'><style type="text/css">#modify_div {position: fixed;right: 0px;top: 60px;	background-color: #000000;	height: 30px;width: 100px;border-top-width: 2px;border-right-width: 2px;border-bottom-width: 2px;border-left-width: 2px;border-top-style: dashed;border-right-style: none;border-bottom-style: dashed;border-left-style: dashed;border-top-color: #333333;border-right-color: #333333;border-bottom-color: #333333;border-left-color: #333333;}#modify {display: block;position: fixed;right: 23px;top: 70px;color: #FFFFFF;text-decoration: none;font-size: 12px;font-weight: bold;}#modify:hover {text-decoration: underline;}</style><div id="modify_div"></div><a href="https://github.com/jialechan/redis-doc-cn/edit/gh-pages/cn/topics/cluster-spec.html" id="modify" target="_blank">修改本页</a><script>$('#modify_div').css('opacity', '0.6');	</script>
    <header>
      <div class='container'>
        <a href='../index.html'>
          <img alt='Redis' height='30' src='../images/redis.png' width='93' />
        </a>
        <nav>
          <a href='../commands.html'>Commands</a>
          <a href='../clients.html'>Clients</a>
          <a href='../documentation.html'>Documentation</a>
          <a href='../community.html'>Community</a>
          <a href='../download.html'>Download</a>
          <a href='https://github.com/antirez/redis/issues'>Issues</a>
          <a href='../support.html'>Support</a>
          <a href='license.html'>License</a>
        </nav>
      </div>
    </header>
    <div class='text'>
      <article id='topic'>
        <h1>Redis cluster Specification (work in progress)</h1>
        
        <h2>Redis Cluster goals</h2>
        
        <p>Redis Cluster is a distributed implementation of Redis with the following goals, in order of importance in the design:</p>
        
        <ul>
        <li>High performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values.</li>
        <li>Acceptable degree of write safety: the system tries (in a best-effort way) to retain all the writes originating from clients connected with the majority of the master nodes. Usually there are small windows where acknowledged writes can be lost. Windows to lose acknowledged writes are larger when clients are in a minority partition.</li>
        <li>Availability: Redis Cluster is able to survive to partitions where the majority of the master nodes are reachable and there is at least a reachable slave for every master node that is no longer reachable.</li>
        </ul>
        
        <p>What is described in this document is implemented in the <code>unstable</code> branch of the Github Redis repository. Redis Cluster has now entered the beta stage, so new betas are released every month and can be found in the <a href="../download.html">download page</a> of the Redis web site.</p>
        
        <h2>Implemented subset</h2>
        
        <p>Redis Cluster implements all the single keys commands available in the
        non distributed version of Redis. Commands performing complex multi key
        operations like Set type unions or intersections are implemented as well
        as long as the keys all belong to the same node.</p>
        
        <p>Redis Cluster implements a concept called <strong>hash tags</strong> that can be used
        in order to force certain keys to be stored in the same node. However during
        manual reshardings multi-key operations may become unavailable for some time
        while single keys operations are always available.</p>
        
        <p>Redis Cluster does not support multiple databases like the stand alone version
        of Redis, there is just database 0, and <a href="../commands/select.html">SELECT</a> is not allowed.</p>
        
        <h2>Clients and Servers roles in the Redis cluster protocol</h2>
        
        <p>In Redis cluster nodes are responsible for holding the data,
        and taking the state of the cluster, including mapping keys to the right nodes.
        Cluster nodes are also able to auto-discover other nodes, detect non working
        nodes, and performing slave nodes election to master when needed.</p>
        
        <p>To perform their tasks all the cluster nodes are connected using a
        TCP bus and a binary protocol (the <strong>cluster bus</strong>).
        Every node is connected to every other node in the cluster using the cluster
        bus. Nodes use a gossip protocol to propagate information about the cluster
        in order to discover new nodes, to send ping packets to make sure all the
        other nodes are working properly, and to send cluster messages needed to
        signal specific conditions. The cluster bus is also used in order to
        propagate Pub/Sub messages across the cluster.</p>
        
        <p>Since cluster nodes are not able to proxy requests clients may be redirected
        to other nodes using redirections errors <code>-MOVED</code> and <code>-ASK</code>.
        The client is in theory free to send requests to all the nodes in the cluster,
        getting redirected if needed, so the client is not required to take the
        state of the cluster. However clients that are able to cache the map between
        keys and nodes can improve the performance in a sensible way.</p>
        
        <h2>Write safety</h2>
        
        <p>Redis Cluster uses asynchronous replication between nodes, and last-elected-master-dataset-wins implicit merge function, so there are always windows when it is possible to lose writes during partitions. However these windows are very different in the case of a client that is connected to the majority of masters, and a client that is connected to the minority of masters.</p>
        
        <p>Redis Cluster tries hard to retain all the writes that are performed by clients connected to the majority of masters, with two exceptions:</p>
        
        <p>1) A write may reach a master, but while the master may be able to reply to the client, the write may not be propagated to slaves via the asynchronous replication used between master and slave nodes. If the master dies without the write reaching the slaves, the write is lost forever in case the master is unreachable for a long enough period that one of its slaves is promoted.</p>
        
        <p>2) Another theoretically possible failure mode where writes are lost is the following:</p>
        
        <ul>
        <li>A master is unreachable because of a partition.</li>
        <li>It gets failed over by one of its slaves.</li>
        <li>After some time it may be reachable again.</li>
        <li>A client with a not updated routing table may write to it before the master is converted to a slave (of the new master) by the cluster.</li>
        </ul>
        
        <p>The second failure mode is unlikely to happen because master nodes not able to communicate with the majority of the other masters for enough time to be failed over, no longer accept writes, and when the partition is fixed writes are still refused for a small amount of time to allow other nodes to inform about configuration changes.</p>
        
        <p>Redis Cluster loses a non trivial amount of writes on partitions where there is a minority of masters and at least one or more clients, since all the writes sent to the masters may potentially get lost if the masters are failed over in the majority side.</p>
        
        <p>Specifically, for a master to be failed over, it must be not reachable by the majority of masters for at least <code>NODE_TIMEOUT</code>, so if the partition is fixed before that time, no write is lost. When the partition lasts for more than <code>NODE_TIMEOUT</code>, the minority side of the cluster will start refusing writes as soon as <code>NODE_TIMEOUT</code> time has elapsed, so there is a maximum window after which the minority becomes no longer available, hence no write is accepted and lost after that time.</p>
        
        <h2>Availability</h2>
        
        <p>Redis Cluster is not available in the minority side of the partition. In the majority side of the partition assuming that there are at least the majority of masters and a slave for every unreachable master, the cluster returns available after <code>NODE_TIMEOUT</code> time, plus a few more seconds required for a slave to get elected and failover its master.</p>
        
        <p>This means that Redis Cluster is designed to survive to failures of a few nodes in the cluster, but is not a suitable solution for applications that require availability in the event of large net splits.</p>
        
        <p>In the example of a cluster composed of N master nodes where every node has a single slave, the majority side of the cluster will remain available as long as a single node is partitioned away, and will remain available with a probability of <code>1-(1/(N*2-1))</code> when two nodes are partitioned away (after the first node fails we are left with <code>N*2-1</code> nodes in total, and the probability of the only master without a replica to fail is <code>1/(N*2-1))</code>.</p>
        
        <p>For example in a cluster with 5 nodes and a single slave per node, there is a <code>1/(5*2-1) = 0.1111</code> probabilities that after two nodes are partitioned away from the majority, the cluster will no longer be available, that is about 11% of probabilities.</p>
        
        <p>Thanks to a Redis Cluster feature called <strong>replicas migration</strong> the Cluster
        availability is improved in many real world scenarios by the fact that
        replicas migrate to orphaned masters (masters no longer having replicas).</p>
        
        <h2>Performance</h2>
        
        <p>In Redis Cluster nodes don&#39;t proxy commands to the right node in charge for a given key, but instead they redirect clients to the right nodes serving a given portion of the key space.</p>
        
        <p>Eventually clients obtain an up to date representation of the cluster and which node serves which subset of keys, so during normal operations clients directly contact the right nodes in order to send a given command.</p>
        
        <p>Because of the use of asynchronous replication, nodes does not wait for other nodes acknowledgment of writes (optional synchronous replication is a work in progress and will be likely added in future releases).</p>
        
        <p>Also, because muliple keys commands are only limited to <em>near</em> keys, data is never moved between nodes if not in case of resharding.</p>
        
        <p>So normal operations are handled exactly as in the case of a single Redis instance. This means that in a Redis Cluster with N master nodes you can expect the same performance as a single Redis instance multiplied by N as the design allows to scale linearly. At the same time the query is usually performed in a single round trip, since clients usually retain persistent connections with the nodes, so latency figures are also the same as the single stand alone Redis node case.</p>
        
        <p>Very high performances and scalability while preserving weak but
        reasonable forms of data safety and availability is the main goal of
        Redis Cluster.</p>
        
        <h2>Why merge operations are avoided</h2>
        
        <p>Redis Cluster design avoids conflicting versions of the same key-value pair in multiple nodes since in the case of the Redis data model this is not always desirable: values in Redis are often very large, it is common to see lists or sorted sets with millions of elements. Also data types are semantically complex. Transferring and merging these kind of values can be a major bottleneck and/or may require a non trivial involvement of application-side logic, additional memory to store meta-data, and so forth.</p>
        
        <h2>Keys distribution model</h2>
        
        <p>The key space is split into 16384 slots, effectively setting an upper limit
        for the cluster size of 16384 nodes (however the suggested max size of
        nodes is in the order of ~ 1000 nodes).</p>
        
        <p>All the master nodes will handle a percentage of the 16384 hash slots.
        When the cluster is <strong>stable</strong>, that means that there is no a cluster
        reconfiguration in progress (where hash slots are moved from one node
        to another) a single hash slot will be served exactly by a single node
        (however the serving node can have one or more slaves that will replace
        it in the case of net splits or failures).</p>
        
        <p>The base algorithm used to map keys to hash slots is the following
        (read the next paragraph for the hash tag exception to this rule):</p>
        
        <pre><code>HASH_SLOT = CRC16(key) mod 16384&#x000A;</code></pre>
        
        <p>The CRC16 is specified as follows:</p>
        
        <ul>
        <li>Name: XMODEM (also known as ZMODEM or CRC-16/ACORN)</li>
        <li>Width: 16 bit</li>
        <li>Poly: 1021 (That is actually x<sup>16</sup> + x<sup>12</sup> + x<sup>5</sup> + 1)</li>
        <li>Initialization: 0000</li>
        <li>Reflect Input byte: False</li>
        <li>Reflect Output CRC: False</li>
        <li>Xor constant to output CRC: 0000</li>
        <li>Output for &quot;123456789&quot;: 31C3</li>
        </ul>
        
        <p>14 out of 16 bit of the output of CRC16 are used (this is why there is
        a modulo 16384 operation in the formula above).</p>
        
        <p>In our tests CRC16 behaved remarkably well in distributing different kind of
        keys evenly across the 16384 slots.</p>
        
        <p><strong>Note</strong>: A reference implementation of the CRC16 algorithm used is available in the Appendix A of this document.</p>
        
        <h2>Keys hash tags</h2>
        
        <p>There is an exception for the computation of the hash slot that is used in order
        to implement <strong>hash tags</strong>. Hash tags are a way to ensure that two keys
        are allocated in the same hash slot. This is used in order to implement
        multi-key operations in Redis Cluster.</p>
        
        <p>In order to implement hash tags, the hash slot is computed in a different
        way. Basically if the key contains a &quot;{...}&quot; pattern only the substring between
        <code>{</code> and <code>}</code> is hashed in order to obtain the hash slot. However since it is
        possible that there are multiple occurrences of <code>{</code> or <code>}</code> the algorithm is
        well specified by the following rules:</p>
        
        <ul>
        <li>If the key contains a <code>{</code> character.</li>
        <li>There is a <code>}</code> character on the right of <code>{</code></li>
        <li>There are one or more characters between the first occurrence of <code>{</code> and the first occurrence of <code>}</code> after the first occurrence of <code>{</code>.</li>
        </ul>
        
        <p>Then instead of hashing the key, only what is between the first occurrence of <code>{</code> and the first occurrence of <code>}</code> on its right are hashed.</p>
        
        <p>Examples:</p>
        
        <ul>
        <li>The two keys <code>{user1000}.following</code> and <code>{user1000}.followers</code> will hash to the same hash slot since only the substring <code>user1000</code> will be hashed in order to compute the hash slot.</li>
        <li>For the key <code>foo{}{bar}</code> the whole key will be hashed as usually since the first occurrence of <code>{</code> is followed by <code>}</code> on the right without characters in the middle.</li>
        <li>For the key <code>foo{{bar}}zap</code> the substring <code>{bar</code> will be hashed, because it is the substring between the first occurrence of <code>{</code> and the first occurrence of <code>}</code> on its right.</li>
        <li>For the key <code>foo{bar}{zap}</code> the substring <code>bar</code> will be hashed, since the algorithm stops at the first valid or invalid (without bytes inside) match of <code>{</code> and <code>}</code>.</li>
        <li>What follows from the algorithm is that if the key starts with <code>{}</code>, it is guaranteed to be hashes as a whole. This is useful when using binary data as key names.</li>
        </ul>
        
        <p>Adding the hash tags exception, the following is an implementation of the <code>HASH_SLOT</code> function in Ruby and C language.</p>
        
        <p>Ruby example code:</p>
        
        <pre><code>def HASH_SLOT(key)&#x000A;    s = key.index &quot;{&quot;&#x000A;    if s&#x000A;        e = key.index &quot;}&quot;,s+1&#x000A;        if e &amp;&amp; e != s+1&#x000A;            key = key[s+1..e-1]&#x000A;        end&#x000A;    end&#x000A;    crc16(key) % 16384&#x000A;end&#x000A;</code></pre>
        
        <p>C example code:</p>
        
        <pre><code>unsigned int HASH_SLOT(char *key, int keylen) {&#x000A;    int s, e; /* start-end indexes of { and } */&#x000A;&#x000A;    /* Search the first occurrence of &#39;{&#39;. */&#x000A;    for (s = 0; s &lt; keylen; s++)&#x000A;        if (key[s] == &#39;{&#39;) break;&#x000A;&#x000A;    /* No &#39;{&#39; ? Hash the whole key. This is the base case. */&#x000A;    if (s == keylen) return crc16(key,keylen) &amp; 16383;&#x000A;&#x000A;    /* &#39;{&#39; found? Check if we have the corresponding &#39;}&#39;. */&#x000A;    for (e = s+1; e &lt; keylen; e++)&#x000A;        if (key[e] == &#39;}&#39;) break;&#x000A;&#x000A;    /* No &#39;}&#39; or nothing between {} ? Hash the whole key. */&#x000A;    if (e == keylen || e == s+1) return crc16(key,keylen) &amp; 16383;&#x000A;&#x000A;    /* If we are here there is both a { and a } on its right. Hash&#x000A;     * what is in the middle between { and }. */&#x000A;    return crc16(key+s+1,e-s-1) &amp; 16383;&#x000A;}&#x000A;</code></pre>
        
        <h2>Cluster nodes attributes</h2>
        
        <p>Every node has an unique name in the cluster. The node name is the
        hex representation of a 160 bit random number, obtained the first time a
        node is started (usually using /dev/urandom).
        The node will save its ID in the node configuration file, and will use the
        same ID forever, or at least as long as the node configuration file is not
        deleted by the system administrator.</p>
        
        <p>The node ID is used to identify every node across the whole cluster.
        It is possible for a given node to change IP and address without any need
        to also change the node ID. The cluster is also able to detect the change
        in IP/port and reconfigure broadcast the information using the gossip
        protocol running over the cluster bus.</p>
        
        <p>Every node has other associated information that all the other nodes
        know:</p>
        
        <ul>
        <li>The IP address and TCP port where the node is located.</li>
        <li>A set of flags.</li>
        <li>A set of hash slots served by the node.</li>
        <li>Last time we sent a ping packet using the cluster bus.</li>
        <li>Last time we received a pong packet in reply.</li>
        <li>The time at which we flagged the node as failing.</li>
        <li>The number of slaves of this node.</li>
        <li>The master node ID, if this node is a slave (or 0000000... if it is a master).</li>
        </ul>
        
        <p>Some of this information is available using the <code>CLUSTER NODES</code> command that
        can be sent to all the nodes in the cluster, both master and slave nodes.</p>
        
        <p>The following is an example of output of <code>CLUSTER NODES</code> sent to a master
        node in a small cluster of three nodes.</p>
        
        <pre><code>$ redis-cli cluster nodes&#x000A;d1861060fe6a534d42d8a19aeb36600e18785e04 127.0.0.1:6379 myself - 0 1318428930 connected 0-1364&#x000A;3886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 connected 1365-2729&#x000A;d289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 connected 2730-4095&#x000A;</code></pre>
        
        <p>In the above listing the different fields are in order: node id, address:port, flags, last ping sent, last pong received, link state, slots.</p>
        
        <h2>The Cluster bus</h2>
        
        <p>Every Redis Cluster node has an additional TCP port in order to receive
        incoming connections from other Redis Cluster nodes. This port is at a fixed
        offset compared to the normal TCP port used to receive incoming connections
        from clients. To obtain the Redis Cluster port, 10000 should be added to
        the normal commands port, so for example if a Redis node is listening for
        client connections to port 6379, the Cluster bus port 16379 will also be
        opened.</p>
        
        <p>Node to Node communication happens exclusively using the Cluster bus and
        the Cluster bus protocol, which is a binary protocol composed of frames
        of different types and sizes. The Cluster bus binary protocol is not
        publicly documented since it is not indented for external software devices
        to talk with Redis Cluster nodes using this protocol. However you can
        obtain more details about the Cluster bus protocol by reading the
        <code>cluster.h</code> and <code>cluster.c</code> files in the Redis Cluster source code.</p>
        
        <h2>Cluster topology</h2>
        
        <p>Redis cluster is a full mesh where every node is connected with every other node using a TCP connection.</p>
        
        <p>In a cluster of N nodes, every node has N-1 outgoing TCP connections, and N-1 incoming connections.</p>
        
        <p>These TCP connections are kept alive all the time and are not created on demand.
        When a node expects an pong reply in response to a ping in the cluster bus, before to wait for enough time to mark the node as unreachable, it will try to
        refresh the connection with the node by reconnecting from scratch.</p>
        
        <p>While Redis Cluster nodes form a full mesh, nodes use a gossip protocol and
        a configuration update mechanism in order to avoid exchanging too many
        packets between nodes during normal conditions.</p>
        
        <h2>Nodes handshake</h2>
        
        <p>Nodes always accept connection in the cluster bus port, and even reply to
        pings when received, even if the pinging node is not trusted.
        However all the other packets will be discarded by the node if the node
        is not considered part of the cluster.</p>
        
        <p>A node will accept another node as part of the cluster only in two ways:</p>
        
        <ul>
        <li><p>If a node will present itself with a <code>MEET</code> message. A meet message is exactly
        like a <a href="../commands/ping.html">PING</a> message, but forces the receiver to accept the node as part of
        the cluster. Nodes will send <code>MEET</code> messages to other nodes <strong>only if</strong> the system administrator requests this via the following command:</p>
        
        <p>CLUSTER MEET ip port</p></li>
        <li><p>A node will also register another node as part of the cluster if a node that is already trusted will gossip about this other node. So if A knows B, and B knows C, eventually B will send gossip messages to A about C. When this happens, A will register C as part of the network, and will try to connect with C.</p></li>
        </ul>
        
        <p>This means that as long as we join nodes in any connected graph, they&#39;ll eventually form a fully connected graph automatically. This means that basically the cluster is able to auto-discover other nodes, but only if there is a trusted relationship that was forced by the system administrator.</p>
        
        <p>This mechanism makes the cluster more robust but prevents that different Redis clusters will accidentally mix after change of IP addresses or other network related events.</p>
        
        <p>All the nodes actively try to connect to all the other known nodes if the link is down.</p>
        
        <h2>MOVED Redirection</h2>
        
        <p>A Redis client is free to send queries to every node in the cluster, including
        slave nodes. The node will analyze the query, and if it is acceptable
        (that is, only a single key is mentioned in the query) it will see what
        node is responsible for the hash slot where the key belongs.</p>
        
        <p>If the hash slot is served by the node, the query is simply processed, otherwise
        the node will check its internal hash slot -&gt; node ID map and will reply
        to the client with a MOVED error.</p>
        
        <p>A MOVED error is like the following:</p>
        
        <pre><code>GET x&#x000A;-MOVED 3999 127.0.0.1:6381&#x000A;</code></pre>
        
        <p>The error includes the hash slot of the key (3999) and the ip:port of the
        instance that can serve the query. The client need to reissue the query
        to the specified ip address and port. Note that even if the client waits
        a long time before reissuing the query, and in the meantime the cluster
        configuration changed, the destination node will reply again with a MOVED
        error if the hash slot 3999 is now served by another node.</p>
        
        <p>So while from the point of view of the cluster nodes are identified by
        IDs we try to simply our interface with the client just exposing a map
        between hash slots and Redis nodes identified by ip:port pairs.</p>
        
        <p>The client is not required to, but should try to memorize that hash slot
        3999 is served by 127.0.0.1:6381. This way once a new command needs to
        be issued it can compute the hash slot of the target key and pick the
        right node with higher chances.</p>
        
        <p>Note that when the Cluster is stable, eventually all the clients will obtain
        a map of hash slots -&gt; nodes, making the cluster efficient, with clients
        directly addressing the right nodes without redirections nor proxies or
        other single point of failure entities.</p>
        
        <p>A client should be also able to handle -ASK redirections that are described
        later in this document.</p>
        
        <h2>Cluster live reconfiguration</h2>
        
        <p>Redis cluster supports the ability to add and remove nodes while the cluster
        is running. Actually adding or removing a node is abstracted into the same
        operation, that is, moving an hash slot from a node to another.</p>
        
        <ul>
        <li>To add a new node to the cluster an empty node is added to the cluster and some hash slot is moved from existing nodes to the new node.</li>
        <li>To remove a node from the cluster the hash slots assigned to that node are moved to other existing nodes.</li>
        </ul>
        
        <p>So the core of the implementation is the ability to move slots around.
        Actually from a practical point of view an hash slot is just a set of keys, so
        what Redis cluster really does during <em>resharding</em> is to move keys from
        an instance to another instance.</p>
        
        <p>To understand how this works we need to show the <code>CLUSTER</code> subcommands
        that are used to manipulate the slots translation table in a Redis cluster node.</p>
        
        <p>The following subcommands are available:</p>
        
        <ul>
        <li>CLUSTER ADDSLOTS slot1 [slot2] ... [slotN]</li>
        <li>CLUSTER DELSLOTS slot1 [slot2] ... [slotN]</li>
        <li>CLUSTER SETSLOT slot NODE node</li>
        <li>CLUSTER SETSLOT slot MIGRATING node</li>
        <li>CLUSTER SETSLOT slot IMPORTING node</li>
        </ul>
        
        <p>The first two commands, <code>ADDSLOTS</code> and <code>DELSLOTS</code>, are simply used to assign
        (or remove) slots to a Redis node. After the hash slots are assigned they
        will propagate across all the cluster using the gossip protocol.
        The <code>ADDSLOTS</code> command is usually used when a new cluster is configured
        from scratch to assign slots to all the nodes in a fast way.</p>
        
        <p>The <code>SETSLOT</code> subcommand is used to assign a slot to a specific node ID if
        the <code>NODE</code> form is used. Otherwise the slot can be set in the two special
        states <code>MIGRATING</code> and <code>IMPORTING</code>:</p>
        
        <ul>
        <li>When a slot is set as MIGRATING, the node will accept all the requests
        for queries that are about this hash slot, but only if the key in question
        exists, otherwise the query is forwarded using a <code>-ASK</code> redirection to the
        node that is target of the migration.</li>
        <li>When a slot is set as IMPORTING, the node will accept all the requests
        for queries that are about this hash slot, but only if the request is
        preceded by an ASKING command. Otherwise if not ASKING command was given
        by the client, the query is redirected to the real hash slot owner via
        a <code>-MOVED</code> redirection error.</li>
        </ul>
        
        <p>At first this may appear strange, but now we&#39;ll make it more clear.
        Assume that we have two Redis nodes, called A and B.
        We want to move hash slot 8 from A to B, so we issue commands like this:</p>
        
        <ul>
        <li>We send B: CLUSTER SETSLOT 8 IMPORTING A</li>
        <li>We send A: CLUSTER SETSLOT 8 MIGRATING B</li>
        </ul>
        
        <p>All the other nodes will continue to point clients to node &quot;A&quot; every time
        they are queried with a key that belongs to hash slot 8, so what happens
        is that:</p>
        
        <ul>
        <li>All the queries about already existing keys are processed by &quot;A&quot;.</li>
        <li>All the queries about non existing keys in A are processed by &quot;B&quot;.</li>
        </ul>
        
        <p>This way we no longer create new keys in &quot;A&quot;.
        In the meantime, a special client that is called <code>redis-trib</code> and is
        the Redis cluster configuration utility will make sure to migrate existing
        keys from A to B. This is performed using the following command:</p>
        
        <pre><code>CLUSTER GETKEYSINSLOT slot count&#x000A;</code></pre>
        
        <p>The above command will return <code>count</code> keys in the specified hash slot.
        For every key returned, redis-trib sends node A a <a href="../commands/migrate.html">MIGRATE</a> command, that
        will migrate the specified key from A to B in an atomic way (both instances
        are locked for the time needed to migrate a key so there are no race
        conditions). This is how <a href="../commands/migrate.html">MIGRATE</a> works:</p>
        
        <pre><code>MIGRATE target_host target_port key target_database id timeout&#x000A;</code></pre>
        
        <p><a href="../commands/migrate.html">MIGRATE</a> will connect to the target instance, send a serialized version of
        the key, and once an OK code is received will delete the old key from its own
        dataset. So from the point of view of an external client a key either exists
        in A or B in a given time.</p>
        
        <p>In Redis cluster there is no need to specify a database other than 0, but
        <a href="../commands/migrate.html">MIGRATE</a> can be used for other tasks as well not involving Redis cluster so
        it is a general enough command.
        <a href="../commands/migrate.html">MIGRATE</a> is optimized to be as fast as possible even when moving complex
        keys such as long lists, but of course in Redis cluster reconfiguring the
        cluster where big keys are present is not considered a wise procedure if
        there are latency constraints in the application using the database.</p>
        
        <h2>ASK redirection</h2>
        
        <p>In the previous section we briefly talked about ASK redirection, why we
        can&#39;t simply use the MOVED redirection? Because while MOVED means that
        we think the hash slot is permanently served by a different node and the
        next queries should be tried against the specified node, ASK means to
        only ask the next query to the specified node.</p>
        
        <p>This is needed because the next query about hash slot 8 can be about the
        key that is still in A, so we always want that the client will try A and
        then B if needed. Since this happens only for one hash slot out of 16384
        available the performance hit on the cluster is acceptable.</p>
        
        <p>However we need to force that client behavior, so in order to make sure
        that clients will only try slot B after A was tried, node B will only
        accept queries of a slot that is set as IMPORTING if the client send the
        ASKING command before sending the query.</p>
        
        <p>Basically the ASKING command set a one-time flag on the client that forces
        a node to serve a query about an IMPORTING slot.</p>
        
        <p>So the full semantics of the ASK redirection is the following, from the
        point of view of the client.</p>
        
        <ul>
        <li>If ASK redirection is received send only the query in object to the specified node.</li>
        <li>Start the query with the ASKING command.</li>
        <li>Don&#39;t update local client tables to map hash slot 8 to B for now.</li>
        </ul>
        
        <p>Once the hash slot 8 migration is completed, A will send a MOVED message and
        the client may permanently map hash slot 8 to the new ip:port pair.
        Note that however if a buggy client will perform the map earlier this is not
        a problem since it will not send the ASKING command before the query and B
        will redirect the client to A using a MOVED redirection error.</p>
        
        <h2>Clients first connection and handling of redirections.</h2>
        
        <p>While it is possible to have a Redis Cluster client implementation that does not
        takes the slots configuration (the map between slot numbers and addresses of
        nodes serving it) in memory, and only works contacting random nodes waiting to
        be redirected, such a client would be very inefficient.</p>
        
        <p>Redis Cluster clients should try to be smart enough to memorize the slots
        configuration. However this configuration does not <em>require</em> to be updated,
        since contacting the wrong node will simply result in a redirection.</p>
        
        <p>Clients usually need to fetch a complete list of slots and mapped node
        addresses in two different moments:</p>
        
        <ul>
        <li>At startup in order to populate the initial slots configuration.</li>
        <li>When a <code>MOVED</code> redirection is received.</li>
        </ul>
        
        <p>Note that a client may handle the <code>MOVED</code> redirection updating just the moved
        slot in its table, however this is usually not efficient since often the
        configuration of multiple slots is modified at once (for example if a slave
        is promoted to master, all the slots served by the old master will be remapped).
        It is much simpler to react to a <code>MOVED</code> redirection fetching the full map
        of slots - nodes from scratch.</p>
        
        <p>In order to retrieve the slots configuration Redis Cluster offers (starting
        with 3.0.0 beta-7) an alternative to the <code>CLUSTER NODES</code> command that does not
        require parsing, and only provides the information strictly needed to clients.</p>
        
        <p>The new command is called <code>CLUSTER SLOTS</code> and provides an array of slots
        ranges, and the associated master and slave nodes serving the specified range.</p>
        
        <p>The following is an example of output of <code>CLUSTER SLOTS</code>:</p>
        
        <pre><code>127.0.0.1:7000&gt; cluster slots&#x000A;1) 1) (integer) 5461&#x000A;   2) (integer) 10922&#x000A;   3) 1) &quot;127.0.0.1&quot;&#x000A;      2) (integer) 7001&#x000A;   4) 1) &quot;127.0.0.1&quot;&#x000A;      2) (integer) 7004&#x000A;2) 1) (integer) 0&#x000A;   2) (integer) 5460&#x000A;   3) 1) &quot;127.0.0.1&quot;&#x000A;      2) (integer) 7000&#x000A;   4) 1) &quot;127.0.0.1&quot;&#x000A;      2) (integer) 7003&#x000A;3) 1) (integer) 10923&#x000A;   2) (integer) 16383&#x000A;   3) 1) &quot;127.0.0.1&quot;&#x000A;      2) (integer) 7002&#x000A;   4) 1) &quot;127.0.0.1&quot;&#x000A;      2) (integer) 7005&#x000A;</code></pre>
        
        <p>The first two sub-elements of every element of the returned array are the
        start-end slots of the range, the additional elements represent address-port
        pairs. The first address-port pair is the master serving the slot, and the
        additional address-port pairs are all the slaves serving the same slot
        that are not in an error condition (the FAIL flag is not set).</p>
        
        <p>For example the first element of the output says that slots from 5461 to 10922
        (start and end included) are served by 127.0.0.1:7001, and it is possible
        to scale read-only load contacting the slave at 127.0.0.1:7004.</p>
        
        <p><code>CLUSTER SLOTS</code> does not guarantee to return ranges that will cover all the
        16k slots if the cluster is misconfigured, so clients should initialize the
        slots configuration map filling the target nodes with NULL objects, and
        report an error if the user will try to execute commands about keys
        that belong to misconfigured (unassigned) slots.</p>
        
        <p>However before to return an error to the caller, when a slot is found to be
        be unassigned, the client should try to fetch the slots configuration
        again to check if the cluster is now configured properly.</p>
        
        <h2>Multiple keys operations</h2>
        
        <p>Using hash tags clients are free to use multiple-keys operations.
        For example the following operation is valid:</p>
        
        <pre><code>MSET {user:1000}.name Angela {user:1000}.surname White&#x000A;</code></pre>
        
        <p>However multi-key operations become unavailable when a resharding of the
        hash slot the keys are hashing to is being moved form a node to another
        (because of a manual resharding).</p>
        
        <p>More specifically, even during a resharding, the multi-key operations
        targeting keys that all exist and are still all in the same node (either
        the source or destination node) are still available.</p>
        
        <p>Operations about keys that don&#39;t exist or are, during the resharding, split
        between the source and destination node, will generate a <code>-TRYAGAIN</code> error.
        The client can try the operation after some time, or report back the error.</p>
        
        <h2>Scaling reads using slave nodes</h2>
        
        <p>Normally slave nodes will redirect clients to the authoritative master for
        the hash slot involved in a given command, however clients can use slaves
        in order to scale reads using the <code>READONLY</code> command.</p>
        
        <p><code>READONLY</code> tells a Redis cluster slave node that the client is ok reading
        possibly stale data and is not interested in running write queries.</p>
        
        <p>When the connection is in <em>readonly</em> mode, the cluster will send a redirection
        to the client only in the context of an operation involving keys not served
        by the slave&#39;s master node. This may happen because:</p>
        
        <ol>
        <li>The client sent a command about hash slots never served by the master of this slave.</li>
        <li>The cluster was reconfigured (for example resharded) and the slave is no longer able to serve commands for a given hash slot.</li>
        </ol>
        
        <p>When this happens the client should update its hashslot map as explained in
        the previous sections.</p>
        
        <p>The <em>readonly</em> state of the connection can be undoed using the <code>READWRITE</code> command.</p>
        
        <h1>Fault Tolerance</h1>
        
        <h2>Nodes heartbeat and gossip messages</h2>
        
        <p>Nodes in the cluster exchange ping / pong packets.</p>
        
        <p>Usually a node will ping a few random nodes every second so that the total number of ping packets send (and pong packets received) is a constant amount regardless of the number of nodes in the cluster.</p>
        
        <p>However every node makes sure to ping every other node that we don&#39;t either sent a ping or received a pong for longer than half the <code>NODE_TIMEOUT</code> time. Before <code>NODE_TIMEOUT</code> has elapsed, nodes also try to reconnect the TCP link with another node to make sure nodes are not believed to be unreachable only because there is a problem in the current TCP connection.</p>
        
        <p>The amount of messages exchanged can be bigger than <span class="math">O(N) </span>if <code>NODE_TIMEOUT</code> is set to a small figure and the number of nodes (N) is very large, since every node will try to ping every other node for which we don&#39;t have fresh information for half the <code>NODE_TIMEOUT</code> time.</p>
        
        <p>For example in a 100 nodes cluster with a node timeout set to 60 seconds, every node will try to send 99 pings every 30 seconds, with a total amount of pings of 3.3 per second, that multiplied for 100 nodes is 330 pings per second in the total cluster.</p>
        
        <p>There are ways to use the gossip information already exchanged by Redis Cluster to reduce the amount of messages exchanged in a significant way. For example we may ping within half <code>NODE_TIMEOUT</code> only nodes that are already reported to be in &quot;possible failure&quot; state (see later) by other nodes, and ping the other nodes that are reported as working only in a best-effort way within the limit of the few packets per second. However in real-world tests large clusters with very small <code>NODE_TIMEOUT</code> settings used to work reliably so this change will be considered in the future as actual deployments of large clusters will be tested.</p>
        
        <h2>Ping and Pong packets content</h2>
        
        <p>Ping and Pong packets contain an header that is common to all the kind of packets (for instance packets to request a vote), and a special Gossip Section that is specific of Ping and Pong packets.</p>
        
        <p>The common header has the following information:</p>
        
        <ul>
        <li>Node ID, that is a 160 bit pseudorandom string that is assigned the first time a node is created and remains the same for all the life of a Redis Cluster node.</li>
        <li>The <code>currentEpoch</code> and <code>configEpoch</code> field, that are used in order to mount the distributed algorithms used by Redis Cluster (this is explained in details in the next sections). If the node is a slave the <code>configEpoch</code> is the last known <code>configEpoch</code> of the master.</li>
        <li>The node flags, indicating if the node is a slave, a master, and other single-bit node information.</li>
        <li>A bitmap of the hash slots served by a given node, or if the node is a slave, a bitmap of the slots served by its master.</li>
        <li>Port: the sender TCP base port (that is, the port used by Redis to accept client commands, add 10000 to this to obtain the cluster port).</li>
        <li>State: the state of the cluster from the point of view of the sender (down or ok).</li>
        <li>The master node ID, if this is a slave.</li>
        </ul>
        
        <p>Ping and pong packets contain a gossip section. This section offers to the receiver a view about what the sender node thinks about other nodes in the cluster. The gossip section only contains information about a few random nodes among the known nodes set of the sender.</p>
        
        <p>For every node added in the gossip section the following fields are reported:</p>
        
        <ul>
        <li>Node ID.</li>
        <li>IP and port of the node.</li>
        <li>Node flags.</li>
        </ul>
        
        <p>Gossip sections allow receiving nodes to get information about the state of other nodes from the point of view of the sender. This is useful both for failure detection and to discover other nodes in the cluster.</p>
        
        <h2>Failure detection</h2>
        
        <p>Redis Cluster failure detection is used to recognize when a master or slave node is no longer reachable by the majority of nodes, and as a result of this event, either promote a slave to the role of master, of when this is not possible, put the cluster in an error state to stop receiving queries from clients.</p>
        
        <p>Every node takes a list of flags associated with other known nodes. There are two flags that are used for failure detection that are called <code>PFAIL</code> and <code>FAIL</code>. <code>PFAIL</code> means <em>Possible failure</em>, and is a non acknowledged failure type. <code>FAIL</code> means that a node is failing and that this condition was confirmed by a majority of masters in a fixed amount of time.</p>
        
        <p><strong>PFAIL flag:</strong></p>
        
        <p>A node flags another node with the <code>PFAIL</code> flag when the node is not reachable for more than <code>NODE_TIMEOUT</code> time. Both master and slave nodes can flag another node as <code>PFAIL</code>, regardless of its type.</p>
        
        <p>The concept of non reachability for a Redis Cluster node is that we have an <strong>active ping</strong> (a ping that we sent for which we still have to get a reply) pending for more than <code>NODE_TIMEOUT</code>, so for this mechanism to work the <code>NODE_TIMEOUT</code> must be large compared to the network round trip time. In order to add reliability during normal operations, nodes will try to reconnect with other nodes in the cluster as soon as half of the <code>NODE_TIMEOUT</code> has elapsed without a reply to a ping. This mechanism ensures that connections are kept alive so broken connections should usually not result into false failure reports between nodes.</p>
        
        <p><strong>FAIL flag:</strong></p>
        
        <p>The <code>PFAIL</code> flag alone is just some local information every node has about other nodes, but it is not used in order to act and is not sufficient to trigger a slave promotion. For a node to be really considered down the <code>PFAIL</code> condition needs to be promoted to a <code>FAIL</code> condition.</p>
        
        <p>As outlined in the node heartbeats section of this document, every node sends gossip messages to every other node including the state of a few random known nodes. So every node eventually receives the set of node flags for every other node. This way every node has a mechanism to signal other nodes about failure conditions they detected.</p>
        
        <p>This mechanism is used in order to escalate a <code>PFAIL</code> condition to a <code>FAIL</code> condition, when the following set of conditions are met:</p>
        
        <ul>
        <li>Some node, that we&#39;ll call A, has another node B flagged as <code>PFAIL</code>.</li>
        <li>Node A collected, via gossip sections, information about the state of B from the point of view of the majority of masters in the cluster.</li>
        <li>The majority of masters signaled the <code>PFAIL</code> or <code>PFAIL</code> condition within <code>NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT</code> time.</li>
        </ul>
        
        <p>If all the above conditions are true, Node A will:</p>
        
        <ul>
        <li>Mark the node as <code>FAIL</code>.</li>
        <li>Send a <code>FAIL</code> message to all the reachable nodes.</li>
        </ul>
        
        <p>The <code>FAIL</code> message will force every receiving node to mark the node in <code>FAIL</code> state.</p>
        
        <p>Note that <em>the FAIL flag is mostly one way</em>, that is, a node can go from <code>PFAIL</code> to <code>FAIL</code>, but for the <code>FAIL</code> flag to be cleared there are only two possibilities:</p>
        
        <ul>
        <li>The node is already reachable, and it is a slave. In this case the <code>FAIL</code> flag can be cleared as slaves are not failed over.</li>
        <li>The node is already reachable, and it is a master not serving any slot. In this case the <code>FAIL</code> flag can be cleared as masters without slots do not really participate to the cluster, and are waiting to be configured in order to join the cluster.</li>
        <li>The node is already reachable, is a master, but a long time (N times the <code>NODE_TIMEOUT</code>) has elapsed without any detectable slave promotion.</li>
        </ul>
        
        <p><strong>While the <code>PFAIL</code> -&gt; <code>FAIL</code> transition uses a form of agreement, the agreement used is weak:</strong></p>
        
        <p>1) Nodes collect views of other nodes during some time, so even if the majority of master nodes need to &quot;agree&quot;, actually this is just state that we collected from different nodes at different times and we are not sure this state is stable.</p>
        
        <p>2) While every node detecting the <code>FAIL</code> condition will force that condition on other nodes in the cluster using the <code>FAIL</code> message, there is no way to ensure the message will reach all the nodes. For instance a node may detect the <code>FAIL</code> condition and because of a partition will not be able to reach any other node.</p>
        
        <p>However the Redis Cluster failure detection has liveness requirement: eventually all the nodes should agree about the state of a given node even in case of partitions, once the partitions heal. There are two cases that can originate from split brain conditions, either some minority of nodes believe the node is in <code>FAIL</code> state, or a minority of nodes believe the node is not in <code>FAIL</code> state. In both the cases eventually the cluster will have a single view of the state of a given node:</p>
        
        <p><strong>Case 1</strong>: If an actual majority of masters flagged a node as <code>FAIL</code>, for the chain effect every other node will flag the master as <code>FAIL</code> eventually.</p>
        
        <p><strong>Case 2</strong>: When only a minority of masters flagged a node as <code>FAIL</code>, the slave promotion will not happen (as it uses a more formal algorithm that makes sure everybody will know about the promotion eventually) and every node will clear the <code>FAIL</code> state for the <code>FAIL</code> state clearing rules above (no promotion after some time &gt; of N times the <code>NODE_TIMEOUT</code>).</p>
        
        <p><strong>Basically the <code>FAIL</code> flag is only used as a trigger to run the safe part of the algorithm</strong>  for the slave promotion. In theory a slave may act independently and start a slave promotion when its master is not reachable, and wait for the masters to refuse the provide acknowledgment if the master is actually reachable by the majority. However the added complexity of the <code>PFAIL -&gt; FAIL</code> state, the weak agreement, and the <code>FAIL</code> message to force the propagation of the state in the shortest amount of time in the reachable part of the cluster, have practical advantages. Because of this mechanisms usually all the nodes will stop accepting writes about at the same time if the cluster is in an error condition, that is a desirable feature from the point of view of applications using Redis Cluster. Also not needed election attempts, initiated by slaves that can&#39;t reach its master for local problems (that is otherwise reachable by the majority of the other master nodes), are avoided.</p>
        
        <h2>Cluster epoch</h2>
        
        <p>Redis Cluster uses a concept similar to the Raft algorithm &quot;term&quot;. In Redis Cluster the term is called epoch instead, and it is used in order to give an incremental version to events, so that when multiple nodes provide conflicting information, it is possible for another node to understand which state is the most up to date.</p>
        
        <p>The <code>currentEpoch</code> is a 64 bit unsigned number.</p>
        
        <p>At node creation every Redis Cluster node, both slaves and master nodes, set the <code>currentEpoch</code> to 0.</p>
        
        <p>Every time a ping or pong is received from another node, if the epoch of the sender (part of the cluster bus messages header) is greater than the local node epoch, then <code>currentEpoch</code> is updated to the sender epoch.</p>
        
        <p>Because of this semantics eventually all the nodes will agree to the greater epoch in the cluster.</p>
        
        <p>The way this information is used is when the state is changed and a node seeks agreement in order to perform some action.</p>
        
        <p>Currently this happens only during slave promotion, as described in the next section. Basically the epoch is a logical clock for the cluster and dictates whatever a given information wins over one with a smaller epoch.</p>
        
        <h2>Config epoch</h2>
        
        <p>Every master always advertises its <code>configEpoch</code> in ping and pong packets along with a bitmap advertising the set of slots it serves.</p>
        
        <p>The <code>configEpoch</code> is set to zero in masters when a new node is created.</p>
        
        <p>A new <code>configEpoch</code> is created during slave election. Slaves trying to replace
        failing masters increment their epoch and try to get the authorization from
        a majority of masters. When a slave is authorized, a new unique <code>configEpoch</code>
        is created, the slave turns into a master using the new <code>configEpoch</code>.</p>
        
        <p>As explained in the next sections the <code>configEpoch</code> helps to resolve conflicts due to different nodes claiming diverging configurations (a condition that may happen because of network partitions and node failures).</p>
        
        <p>Slave nodes also advertise the <code>configEpoch</code> field in ping and pong packets, but in case of slaves the field represents the <code>configEpoch</code> of its master the last time they exchanged packets. This allows other instances to detect when a slave has an old configuration that needs to be updated (Master nodes will not grant votes to slaves with an old configuration).</p>
        
        <p>Every time the <code>configEpoch</code> changes for some known node, it is permanently stored in the nodes.conf file.</p>
        
        <p>Currently when a node is restarted its <code>currentEpoch</code> is set to the greatest <code>configEpoch</code> of the known nodes. This is not safe in a crash-recovery system model, and the system will be modified in order to store the currentEpoch in the persistent configuration as well.</p>
        
        <h2>Slave election and promotion</h2>
        
        <p>Slave election and promotion is handled by slave nodes, with the help of master nodes that vote for the slave to promote.
        A slave election happens when a master is in <code>FAIL</code> state from the point of view of at least one of its slaves that has the prerequisites in order to become a master.</p>
        
        <p>In order for a slave to promote itself to master, it requires to start an election and win it. All the slaves for a given master can start an election if the master is in <code>FAIL</code> state, however only one slave will win the election and promote itself to master.</p>
        
        <p>A slave starts an election when the following conditions are met:</p>
        
        <ul>
        <li>The slave&#39;s master is in <code>FAIL</code> state.</li>
        <li>The master was serving a non-zero number of slots.</li>
        <li>The slave replication link was disconnected from the master for no longer than a given amount of time, in order to ensure to promote a slave with a reasonable data freshness. This time is user configurable.</li>
        </ul>
        
        <p>In order to be elected the first step for a slave is to increment its <code>currentEpoch</code> counter, and request votes from master instances.</p>
        
        <p>Votes are requested by the slave by broadcasting a <code>FAILOVER_AUTH_REQUEST</code> packet to every master node of the cluster. Then it waits for replies to arrive for a maximum time of two times the <code>NODE_TIMEOUT</code>, but always for at least for 2 seconds.</p>
        
        <p>Once a master voted for a given slave, replying positively with a <code>FAILOVER_AUTH_ACK</code>, it can no longer vote for another slave of the same master for a period of <code>NODE_TIMEOUT * 2</code>. In this period it will not be able to reply to other authorization requests for the same master. This is not needed to guarantee safety, but useful to avoid multiple slaves to get elected (even if with a different <code>configEpoch</code>) about at the same time.</p>
        
        <p>A slave discards all the <code>AUTH_ACK</code> replies that are received having an epoch that is less than the <code>currentEpoch</code> at the time the vote request was sent, in order to never count as valid votes that are about a previous election.</p>
        
        <p>Once the slave receives ACKs from the majority of masters, it wins the election.
        Otherwise if the majority is not reached within the period of two times <code>NODE_TIMEOUT</code> (but always at least 2 seconds), the election is aborted and a new one will be tried again after <code>NODE_TIMEOUT * 4</code> (and always at least 4 seconds).</p>
        
        <p>A slave does not try to get elected as soon as the master is in <code>FAIL</code> state, but there is a little delay, that is computed as:</p>
        
        <pre><code>DELAY = 500 milliseconds + random delay between 0 and 500 milliseconds +&#x000A;        SLAVE_RANK * 1000 milliseconds.&#x000A;</code></pre>
        
        <p>The fixed delay ensures that we wait for the <code>FAIL</code> state to propagate across the cluster, otherwise the slave may try to get elected when the masters are still not aware of the <code>FAIL</code> state, refusing to grant their vote.</p>
        
        <p>The random delay is used to desynchronize slaves so they&#39;ll likely start an election in different moments.</p>
        
        <p>The <code>SLAVE_RANK</code> is the rank of this slave regarding the amount of replication
        stream it processed from the master. Slaves exchange messages when the master
        is failing in order to establish a rank: the slave with the most updated
        replication offset is at rank 0, the second must updated at rank 1, and so forth. In this way the most updated slaves try to get elected before others.</p>
        
        <p>Once a slave wins the election, it starts advertising itself as master in ping and pong packets, providing the set of served slots with a <code>configEpoch</code> set to the <code>currentEpoch</code> at which the election was started.</p>
        
        <p>In order to speedup the reconfiguration of other nodes, a pong packet is broadcasted to all the nodes of the cluster (however nodes not currently reachable will eventually receive a ping or pong packet and will be reconfigured).</p>
        
        <p>The other nodes will detect that there is a new master serving the same slots served by the old master but with a greater <code>configEpoch</code>, and will upgrade the configuration. Slaves of the old master, or the failed over master that rejoins the cluster, will not just upgrade the configuration but will also configure to replicate from the new master.</p>
        
        <h2>Masters reply to slave vote request</h2>
        
        <p>In the previous section it was discussed how slaves try to get elected, this section explains what happens from the point of view of a master that is requested to vote for a given slave.</p>
        
        <p>Masters receive requests for votes in form of <code>FAILOVER_AUTH_REQUEST</code> requests from slaves.</p>
        
        <p>For a vote to be granted the following conditions need to be met:</p>
        
        <ul>
        <li>1) A master only votes a single time for a given epoch, and refuses to vote for older epochs: every master has a lastVoteEpoch field and will refuse to vote again as long as the <code>currentEpoch</code> in the auth request packet is not greater than the lastVoteEpoch. When a master replies positively to an vote request, the lastVoteEpoch is updated accordingly.</li>
        <li>2) A master votes for a slave only if the slave&#39;s master is flagged as <code>FAIL</code>.</li>
        <li>3) Auth requests with a <code>currentEpoch</code> that is less than the master <code>currentEpoch</code> are ignored. Because of this the Master reply will always have the same <code>currentEpoch</code> as the auth request. If the same slave asks again to be voted, incrementing the <code>currentEpoch</code>, it is guaranteed that an old delayed reply from the master can not be accepted for the new vote.</li>
        </ul>
        
        <p>Example of the issue caused by not using this rule:</p>
        
        <p>Master <code>currentEpoch</code> is 5, lastVoteEpoch is 1 (this may happen after a few failed elections)</p>
        
        <ul>
        <li>Slave <code>currentEpoch</code> is 3.</li>
        <li>Slave tries to be elected with epoch 4 (3+1), master replies with an ok with <code>currentEpoch</code> 5, however the reply is delayed.</li>
        <li><p>Slave tries to be elected again, with epoch 5 (4+1), the delayed reply reaches to slave with <code>currentEpoch</code> 5, and is accepted as valid.</p></li>
        <li><p>4) Masters don&#39;t vote a slave of the same master before <code>NODE_TIMEOUT * 2</code> has elapsed since a slave of that master was already voted. This is not strictly required as it is not possible that two slaves win the election in the same epoch, but in practical terms it ensures that normally when a slave is elected it has plenty of time to inform the other slaves avoiding that another slave will win a new election.</p></li>
        <li><p>5) Masters don&#39;t try to select the best slave in any way, simply if the slave&#39;s master is in <code>FAIL</code> state and the master did not voted in the current term, the positive vote is granted. However the best slave is the most likely to start the election and win it before the other slaves.</p></li>
        <li><p>6) When a master refuses to vote for a given slave there is no negative response, the request is simply ignored.</p></li>
        <li><p>7) Masters don&#39;t grant the vote to slaves sending a <code>configEpoch</code> that is less than any <code>configEpoch</code> in the master table for the slots claimed by the slave. Remember that the slave sends the <code>configEpoch</code> of its master, and the bitmap of the slots served by its master. What this means is basically that the slave requesting the vote must have a configuration, for the slots it wants to failover, that is newer or equal the one of the master granting the vote.</p></li>
        </ul>
        
        <h2>Race conditions during slaves election</h2>
        
        <p>This section illustrates how the concept of epoch is used to make the slave promotion process more resistant to partitions.</p>
        
        <ul>
        <li>A master is no longer reachable indefinitely. The master has three slaves A, B, C.</li>
        <li>Slave A wins the election and is promoted as master.</li>
        <li>A partition makes A not available for the majority of the cluster.</li>
        <li>Slave B wins the election and is promoted as master.</li>
        <li>A partition makes B not available for the majority of the cluster.</li>
        <li>The previous partition is fixed, and A is available again.</li>
        </ul>
        
        <p>At this point B is down, and A is available again and will compete with C that will try to get elected in order to fail over B.</p>
        
        <p>Both will eventually claim to be promoted slaves for the same set of hash slots, however the <code>configEpoch</code> they publish will be different, and the C epoch will be greater, so all the other nodes will upgrade their configuration to C.</p>
        
        <p>A itself will detect pings from C serving the same slots with a greater epoch and will reconfigure as a slave of C.</p>
        
        <h2>Rules for server slots information propagation</h2>
        
        <p>An important part of Redis Cluster is the mechanism used to propagate the information about which cluster node is serving a given set of hash slots. This is vital to both the startup of a fresh cluster and the ability to upgrade the configuration after a slave was promoted to serve the slots of its failing master.</p>
        
        <p>Ping and Pong packets that instances continuously exchange contain an header that is used by the sender in order to advertise the hash slots it claims to be responsible for. This is the main mechanism used in order to propagate change, with the exception of a manual reconfiguration operated by the cluster administrator (for example a manual resharding via redis-trib in order to move hash slots among masters).</p>
        
        <p>When a new Redis Cluster node is created, its local slot table, that maps a given hash slot with a given node ID, is initialized so that every hash slot is assigned to nil, that is, the hash slot is unassigned.</p>
        
        <p>The first rule followed by a node in order to update its hash slot table is the following:</p>
        
        <p><strong>Rule 1: If an hash slot is unassigned, and a known node claims it, I&#39;ll modify my hash slot table to associate the hash slot to this node.</strong></p>
        
        <p>Because of this rule, when a new cluster is created, it is only needed to manually assign (using the <code>CLUSTER</code> command, usually via the redis-trib command line tool) the slots served by each master node to the node itself, and the information will rapidly propagate across the cluster.</p>
        
        <p>However this rule is not enough when a configuration update happens because of a slave gets promoted to master after a master failure. The new master instance will advertise the slots previously served by the old slave, but those slots are not unassigned from the point of view of the other nodes, that will not upgrade the configuration if they just follow the first rule.</p>
        
        <p>For this reason there is a second rule that is used in order to rebind an hash slot already assigned to a previous node to a new node claiming it. The rule is the following:</p>
        
        <p><strong>Rule 2: If an hash slot is already assigned, and a known node is advertising it using a <code>configEpoch</code> that is greater than the <code>configEpoch</code> advertised by the current owner of the slot, I&#39;ll rebind the hash slot to the new node.</strong></p>
        
        <p>Because of the second rule eventually all the nodes in the cluster will agree that the owner of a slot is the one with the greatest <code>configEpoch</code> among the nodes advertising it.</p>
        
        <h2>UPDATE messages</h2>
        
        <p>The described system for the propagation of hash slots configurations
        only uses the normal ping and pong messages exchanged by nodes.</p>
        
        <p>It also requires that there is a node that is either a slave or a master
        for a given hash slot and has the updated configuration, because nodes
        send their own configuration in pong and pong packets headers.</p>
        
        <p>However sometimes a node may recover after a partition in a setup where
        it is the only node serving a given hash slot, but with an old configuration.</p>
        
        <p>Example: a given hash slot is served by node A and B. A is the master, and at some point fails, so B is promoted as master. Later B fails as well, and the cluster has no way to recover since there are no more replicas for this hash slot.</p>
        
        <p>However A may recover some time later, and rejoin the cluster with an old configuration in which it was writable as a master. There is no replica that can update its configuration. This is the goal of UPDATE messages: when a node detects that another node is advertising its hash slots with an old configuration, it sends the node an UPDATE message with the ID of the new node serving the slots and the set of hash slots (send as a bitmap) that it is serving.</p>
        
        <p>NOTE: while currently configuration updates via ping / pong and UPDATE share the
        same code path, there is a functional overlap between the two in the way they
        update a configuration of a node with stale information. However the two
        mechanisms are both useful because ping / pong messages after some time are
        able to populate the hash slots routing table of a new node, while UPDATE
        messages are only sent when an old configuration is detected, and only
        cover the information needed to fix the wrong configuration.</p>
        
        <h2>Replica migration</h2>
        
        <p>Redis Cluster implements a concept called <em>replica migration</em> in order to
        improve the availability of the system. The idea is that in a cluster with
        a master-slave setup, if the map between slaves and masters is fixed there
        is limited availability over time if multiple independent failures of single
        nodes happen.</p>
        
        <p>For example in a cluster where every master has a single slave, the cluster
        can continue the operations as long the master or the slave fail, but not
        if both fail the same time. However there is a class of failures, that are
        the independent failures of single nodes caused by hardware or software issues
        that can accumulate over time. For example:</p>
        
        <ul>
        <li>Master A has a single slave A1.</li>
        <li>Master A fails. A1 is promoted as new slave.</li>
        <li>Three hours later A1 fails in an independent manner (not related to the failure of A). No other slave is available for promotion since also node A is still down. The cluster cannot continue normal operations.</li>
        </ul>
        
        <p>If the map between masters and slaves is fixed, the only way to make the cluster
        more resistant to the above scenario is to add slaves to every master, however
        this is costly as it requires more instances of Redis to be executed, more
        memory, and so forth.</p>
        
        <p>An alternative is to create an asymmetry in the cluster, and let the cluster
        layout automatically change over time. For example the cluster may have three
        masters A, B, C. A and B have a single slave each, A1 and B1. However the master
        C is different and has two slaves: C1 and C2.</p>
        
        <p>Replica migration is the process of automatic reconfiguration of a slave
        in order to <em>migrate</em> to a master that has no longer coverage (no working
        slaves). With replica migration the scenario mentioned above turns into the
        following:</p>
        
        <ul>
        <li>Master A fails. A1 is promoted.</li>
        <li>C2 migrates as slave of A1, that is otherwise not backed by any slave.</li>
        <li>Three hours later A1 fails as well.</li>
        <li>C2 is promoted as new master to replace A1.</li>
        <li>The cluster can continue the operations.</li>
        </ul>
        
        <h2>Replica migration algorithm</h2>
        
        <p>The migration algorithm does not use any form of agreement, since the slaves
        layout in a Redis Cluster is not part of the cluster configuration that requires
        to be consistent and/or versioned with config epochs. Instead it uses an
        algorithm to avoid mass-migration of slaves when a master is not backed.
        The algorithm guarantees that eventually, once the cluster configuration is
        stable, every master will be backed by at least one slave.</p>
        
        <p>This is how the algorithm works. To start we need to define what is a
        <em>good slave</em> in this context: a good slave is a slave not in FAIL state
        from the point of view of a given node.</p>
        
        <p>The execution of the algorithm is triggered in every slave that detects that
        there is at least a single master without good slaves. However among all the
        slaves detecting this condition, only a subset should act. This subset is
        actually often a single slave unless different slaves have in a given moment
        a slightly different vision of the failure state of other nodes.</p>
        
        <p>The <em>acting slave</em> is the slave among the masters having the maximum number
        of attached slaves, that is not in FAIL state and has the smallest node ID.</p>
        
        <p>So for example if there are 10 masters with 1 slave each, and 2 masters with
        5 slaves each, the slave that will try to migrate is, among the 2 masters
        having 5 slaves, the one with the lowest node ID. Given that no agreement
        is used, it is possible that when the cluster configuration is not stable,
        a race condition occurs where multiple slaves think to be the non-failing
        slave with the lower node ID (but it is an hard to trigger condition in
        practice). If this happens, the result is multiple slaves migrating to the
        same master, which is harmless. If the race happens in a way that will left
        the ceding master without slaves, as soon as the cluster is stable again
        the algorithm will be re-executed again and will migrate the slave back to
        the original master.</p>
        
        <p>Eventually every master will be backed by at least one slave, however
        normally the behavior is that a single slave migrates from a master with
        multiple slaves to an orphaned master.</p>
        
        <p>The algorithm is controlled by an user-configurable parameter called
        <code>cluster-migration-barrier</code>, that is the number of good slaves a master
        will be left with for a slave to migrate. So for example if this parameter
        is set to 2, a slave will try to migrate only if its master remains with
        two working slaves.</p>
        
        <h2>configEpoch conflicts resolution algorithm</h2>
        
        <p>When new <code>configEpoch</code> values are created via slave promotions during
        failovers, they are guaranteed to be unique.</p>
        
        <p>However during manual reshardings, when an hash slot is migrated from
        a node A to a node B, the resharding program will force B to upgrade
        its configuration to an epoch which is the greatest found in the cluster,
        plus 1 (unless the node is already the one with the greatest configuration
        epoch), without to require for an agreement from other nodes.
        This is needed so that the new slot configuration will win over the old one.</p>
        
        <p>This process happens when the system administator performs a manual
        resharding, however it is possible that when the slot is closed after
        a resharding and the node assigns itself a new configuration epoch,
        at the same time a failure happens, just before the new <code>configEpoch</code> is
        propagated to the cluster. A slave may start a failover and obtain
        the authorization.</p>
        
        <p>This scenario may lead to two nodes having the same <code>configEpoch</code>. There
        are other scenarios as well ending with two nodes having the same <code>configEpoch</code>:</p>
        
        <ul>
        <li>New cluster creation: all nodes start with the same <code>configEpoch</code> of 0.</li>
        <li>Possible software bugs.</li>
        <li>Manual editing of the configurations, filesystem corruptions.</li>
        </ul>
        
        <p>When masters serving different hash slots have the same <code>configEpoch</code>, there
        are no issues, and we are more interested in making sure slaves
        failing over a master have a different and unique configuration epoch.</p>
        
        <p>However manual interventions or more reshardings may change the cluster
        configuration in different ways. The Redis Cluster main liveness property
        is that the slot configuration always converges, so we really want under every
        condition that all the master nodes have a different <code>configEpoch</code>.</p>
        
        <p>In order to enforce this, a conflicts resolution is used in the event
        that two nodes end with the same <code>configEpoch</code>.</p>
        
        <ul>
        <li>IF a master node detects another master node is advertising itself with
        the same <code>configEpoch</code>.</li>
        <li>AND IF the node has a lexicographically smaller Node ID compared to the other node claiming the same <code>configEpoch</code>.</li>
        <li>THEN it increments its <code>currentEpoch</code> by 1, and uses it as the new <code>configEpoch</code>.</li>
        </ul>
        
        <p>If there are any set of nodes with the same <code>configEpoch</code>, all the nodes but the one with the greatest Node ID will move forward, guaranteeing that every node
        will pick an unique configEpoch regardless of what happened.</p>
        
        <p>This mechanism also guarantees that after a fresh cluster is created all
        nodes start with a different <code>configEpoch</code>.</p>
        
        <h2>Nodes reset</h2>
        
        <p>Nodes can be software reset (without restarting them) in order to be reused
        in a different role or in a different cluster. This is useful in normal
        operations, in testing, and in cloud environments where a given node can
        be reprovisioned to join a different set of nodes to enlarge or create a new
        cluster.</p>
        
        <p>In Redis Cluster nodes are reset using the <code>CLUSTER RESET</code> command. The
        command is provided in two variants:</p>
        
        <ul>
        <li><code>CLUSTER RESET SOFT</code></li>
        <li><code>CLUSTER RESET HARD</code></li>
        </ul>
        
        <p>The command must be sent directly to the node to reset, and the default reset
        type if no explicit type is provided is to perform a soft reset.</p>
        
        <p>The following is a list of operations performed by reset:</p>
        
        <ol>
        <li>Soft and hard reset: If the node is a slave, it is turned into a master, and its dataset is discarded. If the node is a master and contains keys the reset operation is aborted.</li>
        <li>Soft and hard reset: All the slots are released, and the manual failover state is reset.</li>
        <li>Soft and hard reset: All the other nodes in the nodes table are removed, so the node no longer knows any other node.</li>
        <li>Hard reset only: <code>currentEpoch</code>, <code>configEpoch</code>, and <code>lastVoteEpoch</code> are set to 0.</li>
        <li>Hard reset only: the Node ID is changed to a new random ID.</li>
        </ol>
        
        <p>Master nodes with non-empty data sets can&#39;t be reset (since normally you want to reshard data to the other nodes), however in special conditions when this is appropriate, like when a cluster is totally destroyed in order to create a new one, <a href="../commands/flushall.html">FLUSHALL</a> must be executed before to proceed with the reset.</p>
        
        <h2>Removing nodes from a cluster</h2>
        
        <p>It is possible to practically remove a node from an existing cluster by
        resharding all its data to other nodes (if it is a master node) and finally
        by shutting it down, however the other nodes will still remember its node
        ID and address, and will attempt to connect with it.</p>
        
        <p>For this reason when a node is removed, we want to also remove its entry
        from all the other nodes tables. This is accomplished by using the
        <code>CLUSTER FORGET &lt;node-id&gt;</code> command.</p>
        
        <p>The command does two things:</p>
        
        <ol>
        <li>It removes the node with the specified node ID from the nodes table.</li>
        <li>It sets a 60 seconds ban to prevent a node with the same node ID to be re-added.</li>
        </ol>
        
        <p>The second operation is needed because Redis Cluster uses gossip in order to auto-discover nodes, so removing the node X from node A, could result into node B to gossip node X to A again. Because of the 60 seconds ban, the Redis Cluster administration tools have 60 seconds in order to remove the node from all the nodes, preventing the re-addition of the node because of auto discovery.</p>
        
        <h1>Publish/Subscribe</h1>
        
        <p>In a Redis Cluster clients can subscribe to every node, and can also
        publish to every other node. The cluster will make sure that publish
        messages are forwarded as needed.</p>
        
        <p>The current implementation will simply broadcast all the publish messages
        to all the other nodes, but at some point this will be optimized either
        using bloom filters or other algorithms.</p>
        
        <h2>Appendix A: CRC16 reference implementation in ANSI C</h2>
        
        <pre><code>/*&#x000A; * Copyright 2001-2010 Georges Menie (www.menie.org)&#x000A; * Copyright 2010 Salvatore Sanfilippo (adapted to Redis coding style)&#x000A; * All rights reserved.&#x000A; * Redistribution and use in source and binary forms, with or without&#x000A; * modification, are permitted provided that the following conditions are met:&#x000A; *&#x000A; *     * Redistributions of source code must retain the above copyright&#x000A; *       notice, this list of conditions and the following disclaimer.&#x000A; *     * Redistributions in binary form must reproduce the above copyright&#x000A; *       notice, this list of conditions and the following disclaimer in the&#x000A; *       documentation and/or other materials provided with the distribution.&#x000A; *     * Neither the name of the University of California, Berkeley nor the&#x000A; *       names of its contributors may be used to endorse or promote products&#x000A; *       derived from this software without specific prior written permission.&#x000A; *&#x000A; * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS&#39;&#39; AND ANY&#x000A; * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED&#x000A; * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE&#x000A; * DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY&#x000A; * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES&#x000A; * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;&#x000A; * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND&#x000A; * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT&#x000A; * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS&#x000A; * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.&#x000A; */&#x000A;&#x000A;/* CRC16 implementation according to CCITT standards.&#x000A; *&#x000A; * Note by @antirez: this is actually the XMODEM CRC 16 algorithm, using the&#x000A; * following parameters:&#x000A; *&#x000A; * Name                       : &quot;XMODEM&quot;, also known as &quot;ZMODEM&quot;, &quot;CRC-16/ACORN&quot;&#x000A; * Width                      : 16 bit&#x000A; * Poly                       : 1021 (That is actually x^16 + x^12 + x^5 + 1)&#x000A; * Initialization             : 0000&#x000A; * Reflect Input byte         : False&#x000A; * Reflect Output CRC         : False&#x000A; * Xor constant to output CRC : 0000&#x000A; * Output for &quot;123456789&quot;     : 31C3&#x000A; */&#x000A;&#x000A;static const uint16_t crc16tab[256]= {&#x000A;    0x0000,0x1021,0x2042,0x3063,0x4084,0x50a5,0x60c6,0x70e7,&#x000A;    0x8108,0x9129,0xa14a,0xb16b,0xc18c,0xd1ad,0xe1ce,0xf1ef,&#x000A;    0x1231,0x0210,0x3273,0x2252,0x52b5,0x4294,0x72f7,0x62d6,&#x000A;    0x9339,0x8318,0xb37b,0xa35a,0xd3bd,0xc39c,0xf3ff,0xe3de,&#x000A;    0x2462,0x3443,0x0420,0x1401,0x64e6,0x74c7,0x44a4,0x5485,&#x000A;    0xa56a,0xb54b,0x8528,0x9509,0xe5ee,0xf5cf,0xc5ac,0xd58d,&#x000A;    0x3653,0x2672,0x1611,0x0630,0x76d7,0x66f6,0x5695,0x46b4,&#x000A;    0xb75b,0xa77a,0x9719,0x8738,0xf7df,0xe7fe,0xd79d,0xc7bc,&#x000A;    0x48c4,0x58e5,0x6886,0x78a7,0x0840,0x1861,0x2802,0x3823,&#x000A;    0xc9cc,0xd9ed,0xe98e,0xf9af,0x8948,0x9969,0xa90a,0xb92b,&#x000A;    0x5af5,0x4ad4,0x7ab7,0x6a96,0x1a71,0x0a50,0x3a33,0x2a12,&#x000A;    0xdbfd,0xcbdc,0xfbbf,0xeb9e,0x9b79,0x8b58,0xbb3b,0xab1a,&#x000A;    0x6ca6,0x7c87,0x4ce4,0x5cc5,0x2c22,0x3c03,0x0c60,0x1c41,&#x000A;    0xedae,0xfd8f,0xcdec,0xddcd,0xad2a,0xbd0b,0x8d68,0x9d49,&#x000A;    0x7e97,0x6eb6,0x5ed5,0x4ef4,0x3e13,0x2e32,0x1e51,0x0e70,&#x000A;    0xff9f,0xefbe,0xdfdd,0xcffc,0xbf1b,0xaf3a,0x9f59,0x8f78,&#x000A;    0x9188,0x81a9,0xb1ca,0xa1eb,0xd10c,0xc12d,0xf14e,0xe16f,&#x000A;    0x1080,0x00a1,0x30c2,0x20e3,0x5004,0x4025,0x7046,0x6067,&#x000A;    0x83b9,0x9398,0xa3fb,0xb3da,0xc33d,0xd31c,0xe37f,0xf35e,&#x000A;    0x02b1,0x1290,0x22f3,0x32d2,0x4235,0x5214,0x6277,0x7256,&#x000A;    0xb5ea,0xa5cb,0x95a8,0x8589,0xf56e,0xe54f,0xd52c,0xc50d,&#x000A;    0x34e2,0x24c3,0x14a0,0x0481,0x7466,0x6447,0x5424,0x4405,&#x000A;    0xa7db,0xb7fa,0x8799,0x97b8,0xe75f,0xf77e,0xc71d,0xd73c,&#x000A;    0x26d3,0x36f2,0x0691,0x16b0,0x6657,0x7676,0x4615,0x5634,&#x000A;    0xd94c,0xc96d,0xf90e,0xe92f,0x99c8,0x89e9,0xb98a,0xa9ab,&#x000A;    0x5844,0x4865,0x7806,0x6827,0x18c0,0x08e1,0x3882,0x28a3,&#x000A;    0xcb7d,0xdb5c,0xeb3f,0xfb1e,0x8bf9,0x9bd8,0xabbb,0xbb9a,&#x000A;    0x4a75,0x5a54,0x6a37,0x7a16,0x0af1,0x1ad0,0x2ab3,0x3a92,&#x000A;    0xfd2e,0xed0f,0xdd6c,0xcd4d,0xbdaa,0xad8b,0x9de8,0x8dc9,&#x000A;    0x7c26,0x6c07,0x5c64,0x4c45,0x3ca2,0x2c83,0x1ce0,0x0cc1,&#x000A;    0xef1f,0xff3e,0xcf5d,0xdf7c,0xaf9b,0xbfba,0x8fd9,0x9ff8,&#x000A;    0x6e17,0x7e36,0x4e55,0x5e74,0x2e93,0x3eb2,0x0ed1,0x1ef0&#x000A;};&#x000A;&#x000A;uint16_t crc16(const char *buf, int len) {&#x000A;    int counter;&#x000A;    uint16_t crc = 0;&#x000A;    for (counter = 0; counter &lt; len; counter++)&#x000A;            crc = (crc&lt;&lt;8) ^ crc16tab[((crc&gt;&gt;8) ^ *buf++)&amp;0x00FF];&#x000A;    return crc;&#x000A;}&#x000A;</code></pre>
      </article>
    </div>
    <footer>
      <p>
        This website is
        <a href="https://github.com/antirez/redis-io">open source software</a>
        developed by <a href="http://citrusbyte.com">Citrusbyte</a>.
        <br> The Redis logo was designed by <a href="http://www.carlosprioglio.com/">Carlos Prioglio</a>. See more <a href="sponsors.html">credits</a>.
      </p>
      <div class='sponsor'>
        Sponsored by
        <a href='http://www.gopivotal.com/products/redis'>
          <img alt='Redis Support' height='25' src='../images/pivotal.png' title='Redis Sponsor' width='99' />
        </a>
      </div>
    </footer>
  </body>
</html>

